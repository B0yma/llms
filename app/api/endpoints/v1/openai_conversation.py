from fastapi import APIRouter, HTTPException, Depends, Header, Response
from app.schemas import ChatRequest
from app.config import settings
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains import LLMChain
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from urllib.request import unquote

from app.permissions import verify_api_key

router = APIRouter()


conversational_memory_length = 10
memory_key = "chat_history"
memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=memory_key, return_messages=True) 

@router.post("/conversation")
async def chat_with_model(
                          chat_request: ChatRequest,
                          api_key: str = Depends(verify_api_key),
                          x_api_key: str = Header(None, alias='x-api-key'),
                          x_openai_api_key: str = Header(None, alias='x-openai-api-key'),
                          ):
    try:
        openai_api_key = x_openai_api_key
        if openai_api_key is None or openai_api_key == '':
                openai_api_key = settings.OPENAI_API_KEY

        # Initialize Groq Langchain chat object and conversation
        openai_chat = ChatOpenAI(
            model_name="gpt-3.5-turbo-0125",
            openai_api_key=openai_api_key
        )
        
        user_question = unquote(chat_request.userPromptUrlEncoded)
        
        system_prompt = unquote(chat_request.systemPromptUrlEncoded)
        
        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),  # This is the persistent system prompt that is always included at the start of the chat.

                MessagesPlaceholder(
                    variable_name = memory_key
                ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # This template is where the user's current input will be injected into the prompt.
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=openai_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            verbose=True,   # Enables verbose output, which can be useful for debugging.
            memory=memory,  # The conversational memory object that stores and manages the conversation history.
        )
        
        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input=user_question)
        message = {'human':user_question,'AI':response}
        memory.save_context(
                {'input':message['human']},
                {'output':message['AI']}
                )
        return Response(status_code=200, content=response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

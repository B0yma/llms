from fastapi import APIRouter, HTTPException, Depends, Header
from app.schemas import ChatRequest
from app.config import settings
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains import LLMChain
from langchain_core.messages import SystemMessage
from langchain_groq import ChatGroq
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

from app.permissions import verify_api_key

router = APIRouter()

# Initialize Groq Langchain chat object and conversation
groq_chat = ChatGroq(
            groq_api_key=settings.GROQ_API_KEY, 
            model_name='llama3-8b-8192'
    )   

conversational_memory_length = 10
memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True) 

@router.post("/conversation")
async def chat_with_model(conversation_id: str,
                          chat_request: ChatRequest,
                          api_key: str = Depends(verify_api_key),
                          x_api_key: str = Header(None, alias='x-api-key')
                          ):
    try:
        
        user_question = '''
        tell me a another joke about harry potter
        '''
        
        system_prompt = '''
        You are a quiz coding API specializing in generating various questions that responds in JSON.
        Your job is to generate questions and answer and output the structured data in JSON.
        The JSON schema should include:
        {
            "$schema": "http://json-schema.org/draft-04/schema#",
            "type": "object",
            "properties": {
                "topic": {
                    "type": "string"
                },
                "questions": {
                    "type": "array",
                    "items": [{
                            "type": "object",
                            "properties": {
                                "question": {
                                    "type": "string"
                                },
                                "correct_answer": {
                                    "type": "string"
                                }
                            },
                            "required": [
                                "question",
                                "correct_answer"
                            ]
                        }
                    ]
                }
            },
            "required": [
                "topic",
                "questions"
            ]
        }
        '''
        
        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),  # This is the persistent system prompt that is always included at the start of the chat.

                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # This template is where the user's current input will be injected into the prompt.
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            verbose=True,   # Enables verbose output, which can be useful for debugging.
            memory=memory,  # The conversational memory object that stores and manages the conversation history.
        )
        
        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input=user_question)
        message = {'human':user_question,'AI':response}
        memory.save_context(
                {'input':message['human']},
                {'output':message['AI']}
                )
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
